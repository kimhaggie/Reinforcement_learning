# Reinforcement_learning

YBIGTA 2019년 2학기 컨퍼런스 때 진행했던 프로젝트입니다.

# 방식
김정학 최종문 김승유 송하룡 조준흠 5명이 팀이었고, 2주간 했습니다.

# 기간
2019년 2학기 말

# 설명
q w o p 이 네키를 가지고 저 운동선수를 최대한 많이 앞으로 가게 하는 인터넷에서 할 수 있는 플래시 게임입니다. 이 게임이 조작은 간단하지만 실제로 플레이해보면 상당히 어렵습니다.
<img src="/imgs/qwop.png" width="80%" height="40%"> <br>
저희는 여기서 사용할 데이터를 두가지로 정했습니다.
<img src="/imgs/qwop_data.png" width="80%" height="40%"> <br>
하나는 현재 몇미터를 갔는지 확인할 수 있는 기록 점수이고, 나머지는 현재 플레이어가 어떤 자세를 취하고 있는지 입니다. <br>
이미지를 숫자로 바꿔주기 위해서 구글에서 제공하는 Tesseract OCR이라는 공개 패키지를 사용했습니다. <br>
<img src="/imgs/ocr.png" width="80%" height="40%"> <br>
캡쳐한 이미지는 840 x 900의 크기입니다. 이를 인풋으로 모두 사용하기에는 너무 크기 때문에 이를 65 x 80의 배열로 바꿔줬습니다.
<img src="/imgs/data.png" width="80%" height="40%"> <br>
마코브 체인의 조건을 이용해서 이전 상태와 현재 상태의 차이만을 인풋으로 사용했습니다.
<img src="/imgs/dif.png" width="80%" height="40%"> <br>

네트워크는 너무 복잡하게 하면 실시간으로 학습을 하기 어려울 것으로 판단되서 히든레이어는 2개로 두었습니다. 네트워크의 아웃풋으로는 qwop키중에 어떤걸 선택했을때 가장 좋은 결과가 나오는지 확률 값을 가지고 있습니다. 알고리즘을 간단히 살펴보면 다음과 같습니다. <br>

	while True:

		for-loop 50번:

			게임실행

			게임 끝나면 정보 저장


		50번 동안의 게임 정보를 토대로 네트워크 학습

# Reinforcement_learning

YBIGTA 2019년 2학기 컨퍼런스 때 진행했던 프로젝트입니다.

# 방식
김정학 최종문 김승유 송하룡 조준흠 5명이 팀이었고, 2주간 했습니다.

# 기간
2019년 2학기 말

# 설명
q w o p 이 네키를 가지고 저 운동선수를 최대한 많이 앞으로 가게 하는 인터넷에서 할 수 있는 플래시 게임입니다. 이 게임이 조작은 간단하지만 실제로 플레이해보면 상당히 어렵습니다.
<img src="/imgs/qwop.png" width="80%" height="40%"> <br>
저희는 여기서 사용할 데이터를 두가지로 정했습니다.
<img src="/imgs/qwop_data.png" width="80%" height="40%"> <br>
하나는 현재 몇미터를 갔는지 확인할 수 있는 기록 점수이고, 나머지는 현재 플레이어가 어떤 자세를 취하고 있는지 입니다. <br>
이미지를 숫자로 바꿔주기 위해서 구글에서 제공하는 Tesseract OCR이라는 공개 패키지를 사용했습니다. 이 숫자가 걸린시간과 함께 Reward가 될 것입니다.<br>
<img src="/imgs/ocr.png" width="80%" height="40%"> <br>
캡쳐한 이미지는 840 x 900의 크기입니다. 이를 인풋으로 모두 사용하기에는 너무 크기 때문에 이를 65 x 80의 배열로 바꿔줬습니다.

<img src="/imgs/data.png" width="80%" height="40%"> <br>
마코브 체인의 조건을 이용해서 이전 상태와 현재 상태의 차이만을 인풋으로 사용했습니다.

<img src="/imgs/dif.png" width="80%" height="40%"> <br>

네트워크는 너무 복잡하게 하면 실시간으로 학습을 하기 어려울 것으로 판단되서 히든레이어는 2개로 두었습니다. 네트워크의 아웃풋으로는 qwop키중에 어떤걸 선택했을때 가장 좋은 결과가 나오는지 확률 값을 가지고 있습니다. 알고리즘을 간단히 살펴보면 다음과 같습니다. <br>

	while True:

		for-loop 50번:

			게임실행

			게임 끝나면 정보 저장


		50번 동안의 게임 정보를 토대로 네트워크 학습
		
게임이 끝나면 저장하는 정보는 Reward에 관한 것이며 두가지 입니다. '얼마나 멀리 갔는지'와 '얼마나 많은 시간이 걸렸는지' <br>
50번의 게임이 끝나면 저장된 정보는 확률에 관한 것입니다. 즉 인풋이 주어 졌을 때, 어떤 확률분포를 가질 때 가장 좋은 결과가 나오는지에 대한 정보를 가지고 있습니다.

# 결과
결과만을 말하면 성공한 프로젝트가 아닙니다. 몇 가지 이유가 있는데, 첫 번째로 플래시 게임을 원도우에서만 실행할 방법을 몰라서 윈도우 즉 로컬에서 작업을 했습니다. 그에 따라 GCP에 비해서 하드웨어가 부족했고, 네트워크를 학습하는 것 뿐만 아니라, forward도 시간이 꽤 걸렸습니다. 또한, 이미지를 캡쳐해서 인풋으로 사용한다는 것이 생각보다 엄청나게 시간이 오래 걸렸습니다. 이를 해결하기 위해서 운동선수의 골격만을 정보로 받아두면 좋을 것 같은데, 이는 저희가 할 수 있는 범위를 넘어서 하지 못했습니다. 두 번째로 모델이 qwop게임에 최적화된 모델이 아닌 것 같습니다. 따라서 학습이 완벽하게 이뤄진 것 같지는 않습니다. 다만 성공한 부분도 있는데, 각 명령어 사이사이마다 시간이 좀 걸리지만, 일단 한번 점수가 높게 나오면 모델이 점수가 높았던 방식으로 계속 하려는 경향을 볼 수 있습니다.
